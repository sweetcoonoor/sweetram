{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification using NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNb4D33Km-Aw",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "We propose to achieve movie genre classification based only on movie poster images. A deep neural network is constructed to jointly describe visual appearance and object information, and classify a given movie poster image into genres. Because a movie may belong to multiple genres, this is a multi-label image classification problem. To facilitate related studies, we collect a large-scale movie poster dataset, associated with various metadata. Based on this dataset, we fine-tune a pre-trained convolutional neural network to extract visual representation and adopt a state-of-the-art framework to detect objects in posters. Two types of information are then integrated by the proposed neural network. In the evaluation, we show that the proposed method yields encouraging performance, which is much better than previous works. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HUGzAP8LcRv",
        "colab_type": "code",
        "outputId": "5bcf6742-34ee-454b-b8e2-ac72c7868941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 38kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 43.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.16.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0) (41.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-2.0.0 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV1WA_dqaQiq",
        "colab_type": "text"
      },
      "source": [
        "# Multi-Label Classification\n",
        "\n",
        "Each sample can belong to more than one class. The CNN will have as well \n",
        "C\n",
        " output neurons. The target vector \n",
        "t\n",
        " can have more than a positive class, so it will be a vector of 0s and 1s with \n",
        "C\n",
        " dimensionality.\n",
        "This task is treated as \n",
        "C\n",
        " different binary \n",
        "(\n",
        "C\n",
        "′\n",
        "=\n",
        "2\n",
        ",\n",
        "t\n",
        "′\n",
        "=\n",
        "0\n",
        " or \n",
        "t\n",
        "′\n",
        "=\n",
        "1\n",
        ")\n",
        " and independent classification problems, where each output neuron decides if a sample belongs to a class or not.\n",
        " \n",
        "(![alt text](https://gombru.github.io/assets/cross_entropy_loss/multiclass_multilabel.png))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI4YHxw-aRCS",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://)# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dObdkGS1Y_b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import glob  \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten,Dense,Conv2D,MaxPool2D,Dropout\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ip5tDrM2xsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing import image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rcdHH_64sR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvMAtYFQ4L3d",
        "colab_type": "code",
        "outputId": "4fcf0b86-8c84-4d21-c28f-c5bd224b1da1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gnQbxDG65ffF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fd024761-de81-4c6f-e2da-97b10ee45653"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qz_gRKxKvlw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "# Create a ZipFile Object and load Movie_poster_dataset.zip in it\n",
        "with ZipFile('/content/drive/My Drive/Movie_Poster_Dataset.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in different directory\n",
        "   zipObj.extractall('/content/Multiclassification_Genre/images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhKCrqb8wbe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "# Create a ZipFile Object and load Movie_poster_Metadata in it\n",
        "with ZipFile('/content/drive/My Drive/Movie_Poster_Metadata.zip', 'r') as zipObj:\n",
        "   # Extract all the contents of zip file in different directory\n",
        "   zipObj.extractall('/content/Multiclassification_Genre/MoviesList')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VQSN6KxI5ffA",
        "colab": {}
      },
      "source": [
        "#The JSON files that are present are not well formated, converting it to well formated file.\n",
        "\n",
        "ff=open(\"/content/Multiclassification_Genre/MoviesList/groundtruth/1980.txt\",\"r\")\n",
        "Movies=ff.read()\n",
        "comma=Movies.replace(\"}\",\"},\")\n",
        "newc=comma[:-2]\n",
        "sqaureAdd=\"[\"+newc+\"]\"\n",
        "aaa=sqaureAdd.replace(\"\\n\",\"\")\n",
        "jj=aaa.replace(\"ObjectId(\",\"\").replace(\"\\\")\",\"\\\"\")\n",
        "fin=json.loads(jj)\n",
        "df = pd.DataFrame.from_dict(fin, orient='columns')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2wacuKSx5fe7",
        "colab": {}
      },
      "source": [
        "#Framing the layout\n",
        "cols=df.columns\n",
        "dfFin=pd.DataFrame(columns=cols)\n",
        "cols=df.columns\n",
        "dfFin=pd.DataFrame(columns=cols)\n",
        "dfFin\n",
        "dfFin2=dfFin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_AIZNlTE5fe0",
        "colab": {}
      },
      "source": [
        "#Importing all the text files, since there are 2 types of text files available ,\n",
        "#using a exception block to handle both the file formats\n",
        "\n",
        "all_files = glob.glob(\"/content/Multiclassification_Genre/MoviesLis/*.txt\")\n",
        "\n",
        "print((all_files))\n",
        "count=0\n",
        "for files in all_files:\n",
        "  try:\n",
        "    ff=open(files,\"r\",encoding='utf-16')\n",
        "    stringg=ff.read()\n",
        "    comma=Movies.replace(\"}\",\"},\")\n",
        "    newc=comma[:-2]\n",
        "    sqaureAdd=\"[\"+newc+\"]\"\n",
        "    aaa=sqaureAdd.replace(\"\\n\",\"\")\n",
        "    jj=aaa.replace(\"ObjectId(\",\"\").replace(\"\\\")\",\"\\\"\")\n",
        "    fin=json.loads(jj)\n",
        "    df = pd.DataFrame.from_dict(fin, orient='columns')\n",
        "    dfFin=pd.concat([dfFin,df])\n",
        "  except:\n",
        "    ff=open(files,\"r\",encoding='utf-8')\n",
        "    stringg=ff.read()\n",
        "    comma=Movies.replace(\"}\",\"},\")\n",
        "    newc=comma[:-2]\n",
        "    sqaureAdd=\"[\"+newc+\"]\"\n",
        "    aaa=sqaureAdd.replace(\"\\n\",\"\")\n",
        "    jj=aaa.replace(\"ObjectId(\",\"\").replace(\"\\\")\",\"\\\"\")\n",
        "    fin=json.loads(jj)\n",
        "    df = pd.DataFrame.from_dict(fin, orient='columns')\n",
        "    dfFin=pd.concat([dfFin,df])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pc_930-k5fer",
        "colab": {}
      },
      "source": [
        "#Merging both the dataframes\n",
        "Movie_data=pd.concat([dfFin,dfFin2])\n",
        "#Movie_data.head()\n",
        "Movie_data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GTZFtsYp5fen"
      },
      "source": [
        "Data Exploration and Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0dMG59Dl5feZ",
        "colab": {}
      },
      "source": [
        "#Creating a subset of the the total data\n",
        "\n",
        "Movie_Final=Movie_data[['imdbID','Genre',]]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EoI-gfoD5fd7",
        "colab": {}
      },
      "source": [
        "Movie_Final.to_csv('Movie_genre.csv',encoding=\"ISO-8859-1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGhuLGesT-oF",
        "colab_type": "code",
        "outputId": "0dcf318c-0d18-4bb4-87fb-af5758b63c5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "Movie_genre=Movie_Final\n",
        "Movie_genre['Genre']=[elem[0] for elem in Movie_genre['Genre']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlUePIXWkEBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Moviegenrestats=Movie_genre.groupby('Genre').size().reset_index(name='counts')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ryCzDBlJRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Moviegenrestats=Moviegenrestats.sort_values('counts',ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEGosSYYlW2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MovieFilter=list(Moviegenrestats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYjQmLvl_JLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Getting the Movie Posters\n",
        "image_paths = glob.glob(\"/content/Multiclassification_Genre/images/*.jpg\")\n",
        "image_ids = []\n",
        "for path in image_paths:\n",
        "    start = path.rfind(\"/\")+1\n",
        "    end = len(path)-4\n",
        "    image_ids.append(path[start:end])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP3Np8m55-OZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for fn in image_glob:\n",
        "    try:\n",
        "        img_dict[get_id(fn)] = scipy.misc.imread(fn)\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44_-n-Ri6C4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Showing the image ID's\n",
        "def show_img(id):\n",
        "    title = data[data[\"imdbId\"] == int(id)][\"Title\"].values[0]\n",
        "    genre = data[data[\"imdbId\"] == int(id)][\"Genre\"].values[0]\n",
        "    plt.imshow(img_dict[id])\n",
        "    plt.title(\"{} \\n {}\".format(title, genre))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRhaxFT16L1b",
        "colab_type": "text"
      },
      "source": [
        "# Lets start Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAVqYklE6ZYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing function to scale the image…\n",
        "\n",
        "def preprocess(img, size=(150, 101)):\n",
        "    img = scipy.misc.imresize(img, size)\n",
        "    img = img.astype(np.float32)\n",
        "    img = (img / 127.5) - 1.\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtkNl1246khp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prepare the data\n",
        "def prepare_data(data, img_dict, size=(150, 101)):\n",
        "    print(\"Generation dataset...\")\n",
        "    dataset = []\n",
        "    y = []\n",
        "    ids = []\n",
        "    label_dict = {\"word2idx\": {}, \"idx2word\": []}\n",
        "    idx = 0\n",
        "    genre_per_movie = Movie_final[\"Genre\"].apply(lambda x: str(x).split(\"|\"))\n",
        "    for l in [g for d in genre_per_movie for g in d]:\n",
        "        if l in label_dict[\"idx2word\"]:\n",
        "            pass\n",
        "        else:\n",
        "            label_dict[\"idx2word\"].append(l)\n",
        "            label_dict[\"word2idx\"][l] = idx\n",
        "            idx += 1\n",
        "    n_classes = len(label_dict[\"idx2word\"])\n",
        "    print(\"identified {} classes\".format(n_classes))\n",
        "    n_samples = len(img_dict)\n",
        "    print(\"got {} samples\".format(n_samples))\n",
        "    for k in img_dict:\n",
        "        try:\n",
        "            g = Movie_final[Movie_Final[\"imdbId\"] == int(k)][\"Genre\"].values[0].split(\"|\")\n",
        "            img = preprocess(img_dict[k], size)\n",
        "            if img.shape != (150, 101, 3):\n",
        "                continue\n",
        "            l = np.sum([np.eye(n_classes, dtype=\"uint8\")[label_dict[\"word2idx\"][s]] \n",
        "                                                        for s in g], axis=0)\n",
        "            y.append(l)\n",
        "            dataset.append(img)\n",
        "            ids.append(k)\n",
        "        except:\n",
        "            pass\n",
        "    print(\"DONE\")\n",
        "    return dataset, y, label_dict, ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwCgFqWL62LR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SIZE = (150, 101)\n",
        "dataset, y, label_dict, ids =  prepare_data(data, img_dict, size=SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjWi45Do7dws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SIZE = (150, 101)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',\n",
        "                 input_shape=(SIZE[0], SIZE[1], 3)))\n",
        "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPool2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(29, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbTSix748_2V",
        "colab_type": "text"
      },
      "source": [
        "we are using a sigmoid activation function with a multiclass output-layer. The sigmoid gives us independent propabilities for each class. So softmax is not used"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSXj2VHl8u6X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "6729aa48-0caa-4fae-a1af-efc0904797f8"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 148, 99, 32)       896       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 146, 97, 32)       9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 73, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 73, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 71, 46, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 69, 44, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 34, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 34, 22, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 47872)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               6127744   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 29)                3741      \n",
            "=================================================================\n",
            "Total params: 6,197,053\n",
            "Trainable params: 6,197,053\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26GIEQ4R9J9G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "1ed85405-8727-48e0-9c86-8e9c72e9aa46"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='Adam',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTlDntBg9XdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(np.array(dataset[: n]), np.array(y[: n]), batch_size=16, epochs=5,\n",
        "          verbose=1, validation_split=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SStz_Or9jX1",
        "colab_type": "text"
      },
      "source": [
        "**References:**\n",
        "\n",
        "1)https://www.analyticsvidhya.com/blog/2019/04/predicting-movie-genres-nlp-multi-label-classification/\n",
        "2)https://www.depends-on-the-definition.com/classifying-genres-of-movies-by-looking-at-the-poster-a-neural-approach/\n",
        "3)https://towardsdatascience.com/automated-movie-tagging-a-multiclass-classification-problem-721eb7fb70c2\n",
        "4)https://www.youtube.com/watch?v=Vh26kOCra-Y\n",
        "\n"
      ]
    }
  ]
}